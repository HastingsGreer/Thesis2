\newpage
\clearpage
\section*{Overview}
The different sections of the \textbf{supplementary material} cover the following aspects of our \texttt{GradICON} approach.
\begin{itemize}
	\item {\bf Appendix \ref{SupplementaryTheoreticalDerivation}} provides a justification of our noise assumptions, details of the derivation of the regularization properties of \texttt{GradICON}, and additional insights on the convergence behavior of \texttt{GradICON}.
	\item {\bf Appendix \ref{sec:affine_augmentation}} describes our affine data augmentation strategy in detail.
	\item {\bf Appendix \ref{sec:appendix_comparing_to_other_regularizers}} provides detailed comparisons of \texttt{GradICON} to other regularizers, including the loss curve and examples associated with the experiment in Sec.~\ref{subsection:regularizercomp}.
	\item {\bf Appendix \ref{sec:appendix_convergence_experiment}} describes the details of the experiment in Sec.~\ref{sec:exp_convergence_toy_demo} with convergence speed demonstration on \textbf{OAI} dataset.
	\item {\bf Appendix \ref{sec:appendix_sota_comp}} shows an expanded version of Table~\ref{tab:exp_oai} from the main manuscript. In particular, this table provides information on the provenance of these validation results.
	\item {\bf Appendix \ref{sec:model_statistics}} provides details on inference times, memory use, and numbers of parameters for some key learning-based registration approaches.
	\item {\bf Appendix \ref{appendix:visualizations}} shows example registration results for the \textbf{OAI}, \textbf{HCP}, and \textbf{COPDGene} datasets.
	\item {\bf Appendix \ref{Sec:social_impact}} discusses potentials for negative societal impacts of our work.
\end{itemize}
\section{Supplementary material}
\subsection{Analysis details}
\label{SupplementaryTheoreticalDerivation}
\noindent
\textbf{Experiments on the main modeling hypothesis.} The main modeling hypothesis in the implicit regularization analysis of \ref{subsection:analysis}
is that the noise term $n$ can be neglected in the Taylor expansion
$\nabla \Phi^{AB}(\Phi^{BA}_\theta) = \nabla \Phi^{AB}(\Phi^{BA}) + \varepsilon \nabla^2 \Phi^{AB}(\Phi^{BA})( n^{BA}) + o(\varepsilon)$.
In this formula, we argue in the main text that the noise term $n^{BA}$ can be neglected  with respect to $\varepsilon$, which is the scale of the noise on the Jacobian. Indeed, only the low-frequency noise should appear since integration is a low-pass filter, but we expect this low-frequency noise to be dampened by the similarity measure, which is an $L^2$ norm on the images. On the synthetic dataset, we checked that our hypothesis is valid as a first approximation. From a given output of the network $\Phi^{AB}_\theta,\Phi^{BA}_\theta$, we estimated the closest $\Phi^{AB},\Phi^{BA}$ in $L^2$ norm to our data. Although this estimate is certainly biased, it is the first natural estimator to check our assumption. In \ref{fig:NoisesComparisons}, we plot the noise $n^{AB}$ and its corresponding gradient $\nabla n^{AB}$ in one chosen \emph{direction} (indicated by the red arrow in the plots on the right-hand side).
\begin{figure}[htp]
	\centering
	\includegraphics[width=1.\linewidth]{Figures/pdf_gradscale/noisecomparison_afdesign.pdf}
	\caption{Two \emph{finite difference estimations} of the noise $n^{AB}$ and the gradients $\nabla n^{AB}$ on the synthetic dataset. The magnitude of the gradient is an order of magnitude higher which confirms our hypothesis.}
	\label{fig:NoisesComparisons}
\end{figure}
\vskip1ex
\noindent
\textbf{Derivation details.}
Some steps that were omitted in the main text are explained hereafter.
Our main object of interest is the \texttt{GradICON} regularizer
\begin{equation}
	\lreggicon = \left\|\nabla_x [\Phi_\theta^{AB} (\Phi_\theta^{BA}(x))] - \mathbf{I}\right\|_F^2\enspace.
	\label{app:eqn:gradicon}
\end{equation}
In what follows, we use $\nabla$ insteand of $\nabla_x$.
Making the assumption that the neural network will try to be perfectly inverse consistent in order to minimize \ref{app:eqn:gradicon}, but will make some error due to limited capacity or imprecision in the training process or just because of trying to balance the matching term and the inverse consistency, we decompose each neural network output $\Phi_\theta^{AB}, \Phi_\theta^{BA}$ into two components, a perfectly inverse consistent component $\Phi^{AB}$, and random noise $\varepsilon n^{AB}$, \ie,
\begin{equation}
	\lreggicon = \left\| \nabla[\Phi^{AB} (\Phi_\theta^{BA}) + \varepsilon n^{AB}(\Phi_\theta^{BA})] -  \mathbf{I} \right\|^2_F\enspace.
\end{equation}
By applying the chain rule, this can be rewritten as
\begin{equation}
	\begin{split}
		\lreggicon = \bigl\|((\nabla\Phi^{AB})(\Phi_\theta^{BA}) + \\
		\varepsilon (\nabla n^{AB}) (\Phi_\theta^{BA})) \cdot (\nabla\Phi^{BA} + \varepsilon \nabla n^{BA}) - \mathbf{I}\bigr\|_F^2\enspace.
		\label{eqn:postchain}
	\end{split}
\end{equation}
Next, we Taylor-expand the term $\nabla \Phi^{AB}(\Phi_\theta^{BA})$, \ie, $\nabla \Phi^{AB} (\Phi^{BA} + \varepsilon n^{BA})$  with respect to $\varepsilon$, yielding
\begin{equation}
	\begin{split}
		& \nabla \Phi^{AB}(\Phi^{BA}_\theta) = \\
		& \nabla \Phi^{AB}(\Phi^{BA}) + \varepsilon \nabla^2 \Phi^{AB}(\Phi^{BA}) n^{BA} + o(\varepsilon)\enspace,
	\end{split}
\end{equation}
where $\nabla^2\Phi^{AB}(\Phi^{BA})n^{BA}$ is the appropriate tensor product.
It is clear that the last term can be dropped in the limit of small $\varepsilon$. Plugging this approximation into \ref{eqn:postchain},
we get
\begin{equation}
	\begin{split}
		\lreggicon \approx \bigl\| & (
		\nabla \Phi^{AB}(\Phi^{BA}) \\
		& + \varepsilon \nabla^2 \Phi^{AB}(\Phi^{BA}) n^{BA} \\
		& + \varepsilon (\nabla n^{AB}) (\Phi_\theta^{BA}))\\
		& \cdot (\nabla\Phi^{BA} + \varepsilon \nabla n^{BA}) - \mathbf{I}\bigr\|_F^2\enspace.
	\end{split}
\end{equation}
Upon distributing terms,
\begin{equation}
	\begin{split}
		\lreggicon \approx \bigl\|
		\nabla \Phi^{AB}(\Phi^{BA}) \cdot \nabla\Phi^{BA}  \\~+
		\varepsilon \nabla^2 \Phi^{AB}(\Phi^{BA}) n^{BA} \cdot \nabla\Phi^{BA} \\~+
		\varepsilon (\nabla n^{AB}) (\Phi_\theta^{BA}) \cdot \nabla\Phi^{BA}  \\~+
		\nabla \Phi^{AB}(\Phi^{BA}) \cdot \varepsilon \nabla n^{BA} \\~+
		\varepsilon \nabla^2 \Phi^{AB}(\Phi^{BA}) n^{BA} \cdot \varepsilon \nabla n^{BA} \\~+
		\varepsilon (\nabla n^{AB}) (\Phi_\theta^{BA}) \cdot \varepsilon \nabla n^{BA}  \\
		- \mathbf{I}\bigr\|_F^2\enspace.
	\end{split}
\end{equation}
The first term equals $\mathbf{I}$ and so cancels with the last term. Further,
by assuming small $\varepsilon$, we drop terms in $\varepsilon ^2$, yielding
\begin{equation}
	\begin{split}
		\lreggicon \approx \bigl\|
		\varepsilon \nabla^2 \Phi^{AB}(\Phi^{BA}) n^{BA} \cdot \nabla\Phi^{BA} \\ +
		\varepsilon (\nabla n^{AB}) (\Phi_\theta^{BA}) \cdot \nabla\Phi^{BA}  \\ +
		\nabla \Phi^{AB}(\Phi^{BA}) \cdot \varepsilon \nabla n^{BA} \bigr\|_F^2\enspace.
	\end{split}
	\label{app:eqn:partialres0}
\end{equation}
Following arguments above regarding the relative magnitudes of $n$ and $\nabla n$, the first term in \ref{app:eqn:partialres0} can be neglected, and also $\varepsilon$ can be factored out to obtain
\begin{equation}
	\begin{split}
		\lreggicon \approx \varepsilon^2 \| & \nabla n^{AB}( \Phi_\theta^{BA}) \nabla \Phi^{BA} \\
		& + \nabla \Phi^{AB}(\Phi^{BA})  \nabla n^{BA}\|_F^2\enspace.
	\end{split}
\end{equation}
\noindent Now, we justify and then use the approximation
\begin{equation}
	\nabla n^{AB}(\Phi_\theta^{BA}) = \nabla n^{AB}(\Phi^{BA}) + O(\varepsilon)\enspace.
\end{equation}
To proceed, we make the following remarks. Inversion of the map preserves a first-order expansion in $\varepsilon$, \ie,
\begin{equation}\label{EqFormulaInverseFirstOrder}
	[\Phi_\theta^{AB}]^{-1} = \Phi^{BA} -\varepsilon \nabla \Phi^{BA}(n^{AB}(\Phi^{BA})) + o(\varepsilon)\enspace,
\end{equation}
which can be checked by composition. AÂ similar first-order expansion holds for the Jacobian determinant, \ie,
\begin{equation}\label{EqJacobianApproximation}
	\operatorname{Det}(\nabla [\Phi_\theta^{AB}]^{-1}) = \operatorname{Det}(\nabla [\Phi^{AB}]^{-1}) + O(\varepsilon)\enspace.
\end{equation}
As a consequence, for a differentiable function (possibly vector valued) $I$, we have
\begin{equation}
	I([\Phi_\theta^{BA}]^{-1}(x)) = I([\Phi^{BA}]^{-1}(x)) + O(\varepsilon)\,,
\end{equation}
by \ref{EqFormulaInverseFirstOrder} and first-order Taylor expansion of $I$. We combine the three above equations in what follows.
Since $\nabla n^{AB}$ is a white noise, this formula is justified in the following sense. For a given vector-valued differentiable function $I$ on the image domain $\Omega$, we have
\begin{equation}
	\resizebox{\columnwidth}{!}{$
			\begin{split}
				& \int_\Omega \nabla n^{AB}((\Phi_\theta^{BA}(x))) \cdot I(x)\, dx \\ &= \int_\Omega \nabla n^{AB}(x) \cdot I([\Phi_\theta^{BA}]^{-1}(x))\operatorname{Det}(\nabla [\Phi_\theta^{BA}]^{-1}(x))\,\mathrm{d}x \\
				& = \int_\Omega \nabla n^{AB}(x) \cdot I([\Phi_\theta^{BA}]^{-1}(x))\operatorname{Det}(\nabla [\Phi^{BA}]^{-1}(x)) \,\mathrm{d}x + O(\varepsilon)\\
				& = \int_\Omega \nabla n^{AB}(\Phi^{BA})\cdot I(x)\,\mathrm{d}x + O(\varepsilon)\enspace,
			\end{split}$
	}
\end{equation}
where, in the first and the last equation, we used the change of variable formula.
This brings us to
\begin{equation}
	\begin{split}
		\lreggicon \approx \varepsilon^2 \| & \nabla n^{AB}( \Phi^{BA}) \nabla \Phi^{BA} \\
		& + \nabla \Phi^{AB}(\Phi^{BA})  \nabla n^{BA}\|_F^2\enspace,
	\end{split}
\end{equation}
which is \ref{EqExpansion2} in the main text.
We now expand the square to get
\begin{equation}
	\begin{split}
		\lreggicon\,{\approx}\,\varepsilon^2\left(\right. \| \nabla n^{AB}( \Phi^{BA}) \nabla \Phi^{BA} \|_F^2 \\ ~+
		\| \nabla \Phi^{AB}(\Phi^{BA})  \nabla n^{BA}\|_F^2 \\~+
		2 \langle  \nabla n^{AB}(\Phi^{BA}) \nabla \Phi^{BA},\left[\nabla \Phi^{AB}\right](\Phi^{BA})\nabla n^{BA} \rangle_{F}                    \left.\right)
	\end{split}
\end{equation}
and an application of the fact that $\Phi^{AB}$ and $\Phi^{BA}$ are inverses of each other gives
\begin{equation}
	\begin{split}
		& \hspace{-0.8cm}\lreggicon \approx \\
		\hspace{0.15cm}\varepsilon^2
		\Bigl(
		& \left\| \nabla n^{AB}(\Phi^{BA}) \nabla \Phi^{BA}\right\|^2_{F}~+ %\\ &
		\left\| \left[\nabla \Phi^{BA}\right]^{-1}\nabla n^{BA} \right\|^2_{F} \\
		& \, + 2 \langle  \nabla n^{AB}(\Phi^{BA}) \nabla \Phi^{BA},\left[\nabla \Phi^{BA}\right]^{-1}\nabla n^{BA} \rangle_{F}
		\Bigr)\,.
	\end{split}
	\vspace{-0.2cm}
\end{equation}
When \emph{taking expectation} in \ref{EqTakingExpectation} of the main text, the white noise independence assumption implies that
\begin{equation}
	\mathbb{E}[\nabla n^{BA}_i(y) \nabla n^{AB}_j(x)] = 0
\end{equation}
for all coordinates $i,j$ and $x,y \in \Omega$; this explains that the cross-term vanishes. Thus, we arrive at
\begin{equation*}
	\begin{split}
		\mathbb{E}[\lreggicon] \approx \mathbb{E}\Big[\varepsilon^2
		\Bigl(
		& \left\| \nabla n^{AB}(\Phi^{BA}) \nabla \Phi^{BA}\right\|^2_{F}~+ \\
		& \left\| \left[\nabla \Phi^{BA}\right]^{-1}\nabla n^{BA} \right\|^2_{F} \Bigr)\Big]\,.
	\end{split}
	\vspace{-0.2cm}
\end{equation*}
Now, to further simplify \ref{EqTakingExpectation}, we use the fact that
$\mathbb{E}[\nabla n^{AB}_i(x) \nabla n^{AB}_j(x)] = \delta_{ij}$ where $\delta_{ij} = 1$ if $i=j$ and $\delta_{ij} = 0$ if  $i\neq j$. A direct computation already gives the second term of \ref{Equ:regularization_form} from the main text, \ie,
\begin{equation}
	\begin{split}
		\mathbb{E}[\lreggicon] \approx \mathbb{E}\Big[\varepsilon^2
		\Bigl(
		& \left\| \nabla n^{AB}(\Phi^{BA}) \nabla \Phi^{BA}\right\|^2_{F}~+ \\
		& \left\| \left[\nabla \Phi^{BA}\right]^{-1}\right\|^2_{F} \Bigr)\Big]\,.
	\end{split}
\end{equation}
\vskip0.5ex
In order to obtain the first term of \ref{Equ:regularization_form} from \ref{EqTakingExpectation} in the main text, one needs to use a change of variables $y = \Phi^{AB}(x)$ in space, which results in the appearance of the determinant of the Jacobian matrix, denoted by $\operatorname{Det}(\nabla \Phi^{AB})$, and then similarly use the white noise assumption. The first term of \ref{Equ:regularization_form} has a square root since it is put inside the squared Frobenius norm.
Overall, we arrive at \ref{Equ:regularization_form} from the main text, \ie,
\begin{equation}
	\begin{split}
		\mathbb{E}[\lreggicon]  \approx \varepsilon^2 &
		\Biggl(
		\left\|
		\left[\nabla \Phi^{AB}\right]^{-1} \sqrt{\operatorname{Det}(\nabla\Phi^{AB})}
		\right\|_F^2 \\
		& \hspace{0.23cm}
		%\left.
		+ \left\|
		\left[\nabla\Phi^{BA}\right]^{-1}
		\right\|_F^2\Biggr)\enspace.
		%\right)\enspace,
	\end{split}
\end{equation}
\vskip1ex
\noindent
\textbf{\texttt{GradICON} and preconditioning.}
Recall that in our notation $\psi(x) = \Phi^{AB}(\Phi^{BA}(x)) - \operatorname{Id}$. The \texttt{ICON} formulation uses $\| \psi \|_{L^2}^2$, whereas the \texttt{GradICON} formulation uses $\| \nabla \psi\|_{L^2}^2$.
Our goal is to understand the effect of this change on the optimization scheme.
The parameters of the neural networks encoding the map from $A, B$ to $\Phi^{AB}$ are optimized to minimize the overall loss but let us focus on the inverse consistency loss. For each pair $A, B$, automatic differentiation computes the gradient of the loss with respect to $\psi$, which is then backpropagated.  Computing the gradient of the \texttt{GradICON} loss can be done by rewriting
\begin{equation}
	\| \nabla \psi\|_{L^2}^2 = -\langle \psi, \Delta \psi \rangle_{L^2}\enspace,
\end{equation}
where $\Delta$ is the Laplacian. Hence, the gradient is $-2\Delta \psi$, which is also called change of metric or \emph{preconditioning}. This gradient has a particularly clear formulation in Fourier space (denoting by $\hat f(\omega)$ the Fourier transform of $f(x)$) since it reads as
\begin{equation}
	\widehat{ \Delta \psi}(\omega) = -|\omega|^2\hat \psi(\omega)
\end{equation}
and has to be compared with the gradient of the \texttt{ICON} loss which is $\hat \psi$. Low frequencies are thus damped in comparison to high frequencies.
For instance, the gradient flows (steepest descent in cont. time) of the two regularizers are
\begin{equation}
	\begin{split}
		\partial_t \psi(x) & = -\psi(x)\enspace\text{and}\\
		\partial_t \psi(x) & = \Delta \psi(x)\enspace.
	\end{split}
\end{equation}
Please note $\psi(x)$ on the top is the $\psi$ in \texttt{ICON} loss while the one on the bottom corresponds to the $\psi$ in \texttt{GradICON} loss.
%$\partial_t \psi(x) = -\psi(x)$ and $\partial_t \psi(x) = \Delta \psi(x)$;
In Fourier space, these flows become
\begin{equation}
	\begin{split}
		\partial_t\hat{\psi}(\omega) & = -\hat{\psi}(\omega) \enspace\text{and}\\
		\partial_t\hat{\psi}(\omega) & = -|\omega|^2 \hat{\psi}(\omega)\enspace.
	\end{split}
\end{equation}
%$\partial_t\hat{\psi}(\omega) = -\hat{\psi}(\omega)$ and $\partial_t\hat{\psi} = -|\omega|^2 \hat{\psi}(\omega)$. 
Here, the main difference is exponential convergence of the first gradient
flow while for the second one (related to \texttt{GradICON}), the rate of
exponential convergence depends on $| \omega|^2$, \ie, faster convergence
occurring for high spatial frequencies $\omega$. We now still need to
understand why such a preconditioning is beneficial for the overall goal of
the diffeomorphic registration problem. From the discussion above, it is clear
that low-frequency perturbations are less penalized than high-frequency
perturbations. The simplest example is the case of constant perturbations
which are not penalized at all by the penalty on the gradient. \vskip0.5ex The
purpose of inverse consistency is to encourage each one of the transformations
$\Phi^{AB}$ and $\Phi^{BA}$ to be a bijective map. However, a relaxation of
this loss (in the sense of a relaxation of a constraint) which still
encourages invertible maps can be beneficial in the context of diffeomorphic
registration. A possible idea consists in relaxing the constraint of $\psi$ to
be a deformation close to identity, while still being diffeomorphic. As a
consequence, if both $\Phi^{AB}(\Phi^{BA})$ and $\Phi^{BA}(\Phi^{AB})$ are
invertible, then both transformation are also invertible. In fact,
constraining these two compositions to be any diffeomorphism also leads to the
same conclusion.

\vskip0.5ex
Further, it is well-known that perturbation of identity by a map with a small Lipschitz constant remains a diffeomorphism. It is the result of the inverse function theorem, explained in more detail below.
Small variations around identity by a Lipschitz map can be written as $\operatorname{Id} + v$ with $v: \mathbb R^d \to \mathbb R^d$ Lipschitz. We want these perturbations of identity to remain diffeomorphic and, in particular, injective (equivalent to asking for no foldings). A sufficient condition to satisfy injectivity is that $\|v(x) - v(y)\| \leq \varepsilon \|x- y\|$ (\ie, $v$ is $\varepsilon$-Lipschitz) with $\varepsilon <1$. When $v$ is $C^1$, the Lipschitz inequality reduces to saying that $\| \nabla v(x)\| \leq \varepsilon$ for every point $x$ in the domain. In fact, as shown by the inverse function theorem, this condition is also sufficient for $\operatorname{Id} + \varepsilon v$ to be a diffeomorphism.  Recall in our case, the deviation to identity is denoted by $\psi$.
In view of this sufficient condition, one would ideally penalize the maximum value of $\|\nabla \psi(x) \|$.
\vskip0.5ex
To sum up, in order to control the invertibility of $\Phi^{AB}(\Phi^{BA}(x))$, it is better to control $\nabla \psi$ rather than $\psi$ itself.
Last, let us show on a concrete example that the \texttt{ICON} regularizer can be more constrained than the \texttt{GradICON} regularizer. To this end, note that \emph{constant} shifts around identity are still invertible maps but \texttt{ICON}  penalizes too large constant shifts, while \texttt{GradICON} does not at all. Also, having small \texttt{ICON} loss does not guarantee invertibility of the resulting maps.
Indeed, there are maps $v$ with a small $L^2$ norm for which the magnitude of the gradient may be larger than $1$, thereby potentially leading to folds.
Penalizing the maximum value of the norm of the gradient of $\psi$ is better suited to guarantee invertibility when the loss is less than $1$. However, in practice, this loss has a lack of differentiability; using an $L^2$ loss is much simpler, more convenient, and retains some of the nice properties mentioned above.
% \subsection{Scaling $\lambda$ between \texttt{GradICON} and \texttt{ICON}}
% \label{sup:derivation_of_lambda_scaling}
% Another property of gradient inverse consistency is that its value is invariant to the change of resolution. In multi-resolution registration networks where the loss function needs to be computed at different resolutions, the similarity measure is invariant to the resolution and the regularizer balance factor will require tweaking between different resolutions if the regularizer term is variant to the resolution. Otherwise, the regularizer will play less effect on the total loss while the increase of the resolution. 
% We use spatial transformer~\cite{jaderberg2015spatial} for transformation warping. The spatial transformer assumes the transformation field in a standard range $[-1,1]$. Thus, the size of one voxel is $\frac{1}{N}$ with $N$ being the shape of the transformation field along one dimension. The expectation of inverse consistency loss can be seen as $E[\mathcal{L}_{reg}^{ICON}]=C_1*{voxel\_size}=\frac{C_1}{N}$ which will decrease when resolution increases. $\Delta$ in Equ.~\ref{eq:grad_icon_regularizer_expansion} is a small number equal or less than one voxel. Hence, we can notate it as $C_2*{voxel\_size}=\frac{C_2}{N}$. In gradient inverse consistency loss shown in Equ.~\ref{eq:grad_icon_regularizer_expansion}, the $N$ denominator in the deviation will be canceled by $\frac{1}{\Delta^2}$ leading to an expectation invariant to the resolution $N$. [ADD EXPERIMENTS]

\subsection{Affine data augmentation details}
\label{sec:affine_augmentation}
When we train using affine augmentation, first, we sample a new pair of images from the dataset. Then, we randomly choose whether the image is flipped along each axis: these choices are shared between images in the pair. Finally, independently for each image in the pair, we sample a $3 \times 4$ matrix (with each entry i.i.d. from a standard Gaussian, denoted as $\mathcal{N}^{(3, 4)}(0, 1)$) that represents an affine warp using homogeneous coordinates. This produces an augmented image $\hat{I}$, \ie,
\begin{align*}\hat{I}(\Vec{x}) = I\left(\left(
		\begin{bmatrix}
			u_1 & 0   & 0   & 0 \\
			0   & u_2 & 0   & 0 \\
			0   & 0   & u_3 & 0
		\end{bmatrix}
		{+}\, \gamma \cdot \mathcal{N}^{(3, 4)}(0, 1) \right)
	\begin{bmatrix}
			\Vec{x} \\
			1
		\end{bmatrix}\right)
\end{align*}
where $u_i \sim \text{Uniform}\{\pm 1\}$ and $\gamma=0.05$.
\subsection{Comparison to other regularizers}
\label{sec:appendix_comparing_to_other_regularizers}
In \ref{subsection:regularizercomp} of our main text, we conducted a comparison between different regularizers using displacement vector fields (DVF). However, \ref{Fig:exp_comp_regularizer_with_varying_lambda} only shows \emph{aggregated} results. In  \ref{Fig:regularizer_comp_example_of_TC} and \ref{Fig:regularizer_comp_loss_curve_of_TC}, we present
one registration example from the \textbf{Triangles and Circles} and \textbf{DRIVE} data, respectively. \ref{Fig:regularizer_comp_example_of_retina} and \ref{Fig:regularizer_comp_loss_curve_of_retina} show the image similarity measure, the number of folds and the mean of the squared $L_2$-norm on the displacement vector field plotted over training iterations and with varying regularization weight.
\begin{figure*}[b]
	\centering
	\includegraphics[width=.85\linewidth]{Figures/convergence_experiment/circle_various_lambda_comparison_residual_of_warped_horizontal_afdesign.pdf}
	\vspace{0.2cm}
	\caption{Illustration of one \emph{warped source image} and the corresponding transformation maps for different regularizers across varying regularization strengths on {\bf Triangles and Circles}. \emph{Best-viewed in color.}}
	\label{Fig:regularizer_comp_example_of_TC}
\end{figure*}
\begin{figure*}[b]
	\centering
	\includegraphics[width=.85\linewidth]{Figures/convergence_experiment/circle_various_lambda_comparison_loss_curve_horizontal_afdesign.pdf}
	\vspace{0.1cm}
	\caption{Illustration of image (dis)similarity (\ie, $1-\text{LNCC}$), the number of folds (Folds), and the mean of the squared $L^2$ norm of the displacement vector field (Magnitude) for different regularizers and across varying regularization strengths on {\bf Triangles and Circles}. \emph{Best-viewed in color.}}
	\label{Fig:regularizer_comp_loss_curve_of_TC}
\end{figure*}
\begin{figure*}[b]
	\centering
	\includegraphics[width=.80\linewidth]{Figures/convergence_experiment/retina_various_lambda_comparison_residual_of_warped_horizontal_afdesign.pdf}
	\vspace{0.2cm}
	\caption{Illustration of the \emph{residual error} and the corresponding transformation maps between the \emph{warped source image} and the \emph{target image} for different regularizers across varying regularization strengths on {\bf DRIVE}. \emph{Best-viewed in color.}}
	\label{Fig:regularizer_comp_example_of_retina}
\end{figure*}
\begin{figure*}[b]
	\centering
	\includegraphics[width=.85\linewidth]{Figures/convergence_experiment/retina_various_lambda_comparison_loss_curve_horizontal_afdesign.pdf}
	\caption{Illustration of image (dis)similarity (\ie, $1-\text{LNCC}$), the number of folds (Folds), and the mean of the squared $L^2$ norm of the displacement vector field (Magnitude) for different regularizers and across varying regularization strengths on {\bf DRIVE}. \emph{Best-viewed in color.}}
	\label{Fig:regularizer_comp_loss_curve_of_retina}
\end{figure*}

\subsection{Convergence of Gradient Inverse Consistency}
\label{sec:appendix_convergence_experiment}
We conduct the experiment in \ref{sec:exp_convergence_toy_demo} based on our experimental result of \ref{Fig:exp_comp_regularizer_with_varying_lambda}. In particular, we draw a horizontal line in \ref{Fig:exp_comp_regularizer_with_varying_lambda} at $\%|J|=10^{-2}$ and find the closest point to the line on the \texttt{GradICON} and \texttt{ICON} curve. We then plot the loss curves associated with these points in \ref{Fig:exp_convergence_experiment}.
\vskip0.5ex
As can be seen from \ref{Fig:exp_convergence_experiment}, there is a clear convergence speed difference between \texttt{GradICON} and \texttt{ICON} on the two 2D datasets. While a similar study on actual 3D data would be interesting, training 13 models with varying $\lambda$ is computationally challenging.  Nevertheless, to obtain some intuition about convergence speed differences on 3D data, we present in \ref{fig:OAI_curves} the training loss curve, as well as the transform magnitude and the (log) number of folds, of \texttt{GradICON} and \texttt{ICON} corresponding to the \textbf{OAI} dataset in \ref{tab:exp_oai}. \ref{fig:OAI_curves} clearly supports our claim of  faster convergence of models trained with \texttt{GradICON} regularization over models trained with \texttt{ICON} regularization. Further, it can be seen that \texttt{GradICON} exhibits lower similarity loss and shows larger transform magnitudes because it better captures the large deformations in the \textbf{OAI} dataset.
\begin{figure*}[htp]
	\centering
	%\includegraphics[width=.6\linewidth]{Figures/icon oai convergence comparison.png}
	%\includegraphics[width=\columnwidth]{Figures/icon_gradicon_knee_loss/icon_gradicon_knee_comparison_similarity_loss.pdf}
	%\includegraphics[width=.3\linewidth]{Figures/icon_gradicon_knee_loss/icon_gradicon_knee_comparison_transform_magnitude.pdf}
	\includegraphics[width=0.6\textwidth]{Figures/icon_gradicon_knee_loss/OAI_convergence_afdesign.pdf}
	\vspace{0.1cm}
	\caption{Image similarity and the number of folds, plotted over training iterations for the \texttt{ICON} and \texttt{GradICON} (MSE, $\lambda=0.2$) entries in the \textbf{OAI} section of \ref{tab:exp_real_datasets_full}. \texttt{ICON}'s parity in similarity loss early in training is illusory, as unlike our approach it is trained progressively, and so during these iterations, it is still being trained at low resolution. This leads to a lower MSE during this phase, as the MSE demands more precise alignment on higher resolution (and hence not low-pass filtered) input images. Once both networks are training at the final resolution, the values are directly comparable. These results demonstrate the faster convergence rate, regularity, and final performance of \texttt{GradICON}.}
	\label{fig:OAI_curves}
\end{figure*}

\subsection{Details for comparisons in Table~\ref{tab:exp_oai}}
\label{sec:appendix_sota_comp}
\begin{table*}[b]
	\centering
	\begin{small}
		\begin{tabular}{lcccccc}
			\toprule
			\bf Method                                            & \bf Trans.              & $\lreg$                               & $\lsim$     & \bf DICE\, $\uparrow$      & $\%|J|\downarrow$ & Reported by                      \\
			\midrule
			\multicolumn{6}{c}{\bf OAI}                                                                                                                                                                                               \\
			Initial                                               &                         &                                       &             & 7.6\resTS                                                                         \\
			\midrule
			Demons~\cite{vercauteren2009diffeomorphic}            & A,DVF                   & Gaussian                              & MSE         & 63.5\resTS                 & 0.0006            & \cite{shen2019networks}          \\
			SyN~\cite{avants2008symmetric}                        & A,VF                    & Gaussian                              & LNCC        & 65.7\resTS                 & 0.0000            & ~\cite{shen2019networks}         \\
			NiftyReg~\cite{modat2010fast}                         & A,B-Spline              & BE                                    & NMI         & 59.7\resTS                 & 0.0000            & ~\cite{shen2019networks}         \\
			NiftyReg~\cite{modat2010fast}                         & A,B-Spline              & BE                                    & LNCC        & {\bf 67.9}\resTS           & 0.0068
			                                                      & \cite{shen2019networks}                                                                                                                                           \\
			vSVF-opt~\cite{shen2019networks}                      & A,vSVF                  & m-Gauss                               & LNCC        & 67.4\resTS                 & 0.0000            & ~\cite{shen2019networks}         \\ \hline
			VM~\cite{balakrishnan2019voxelmorph}                  & SVF                     & Diff.                                 & MSE         & 46.1\resTS                 & 0.0028            & ~\cite{shen2019networks}         \\
			VM~\cite{balakrishnan2019voxelmorph}                  & A,SVF                   & Diff.                                 & MSE         & 66.1\resTS                 & 0.0013            & ~\cite{shen2019networks}         \\ %39  \\
			AVSM~\cite{shen2019networks}                          & A,vSVF                  & m-Gauss                               & LNCC        & 68.4\resTS                 & 0.0005            & ~\cite{shen2019networks}         \\ %14 \\ %.3  \\
			\texttt{ICON}~\cite{greer2021icon}                    & DVF                     & \texttt{ICON}                         & MSE         & 65.1\resTS                 & 0.0040            & $\ast$                           \\ %118 \\ %.4  \\
			{\textbf{Ours} (MSE, $\lambda{=}0.2$)}                & DVF                     & \cellcolor{black!10}\texttt{GradICON} & MSE         & 69.5\resTS                 & 0.0000            & $\ast$                           \\ %1.3 \\  %1.71
			{\textbf{Ours} (MSE, $\lambda{=}0.2$, Opt.)}          & DVF                     & \cellcolor{black!10}\texttt{GradICON} & MSE         & 70.5\resTS                 & 0.0001            & $\ast$                           \\ %2.0 \\
			\multirow{2}{*}{\textbf{Ours} \emph{(std. protocol)}} & DVF                     & \cellcolor{black!10}\texttt{GradICON} & LNCC        & 70.1$\dagger$              & 0.0261            & $\ast$                           \\ %771  \\  %1.71
			                                                      & DVF                     & \cellcolor{black!10}\texttt{GradICON} & LNCC        & {\bf 71.2}$\ddagger$       & 0.0042            & $\ast$                           \\
			\midrule
			\multicolumn{6}{c}{\bf HCP}                                                                                                                                                                                               \\
			Initial                                               &                         &                                       &             & 53.4\resTS                 &                                                      \\
			\midrule
			FreeSurfer-Affine~\cite{reuter2010highly}             & A                       & \textemdash                           & TB          & 62.1\resTS                 & 0.0000            & $\ast$                           \\
			SyN~\cite{avants2008symmetric}                        & A,VF                    & Gaussian                              & MI          & \bf{75.8}\resTS           & 0.0000            & $\ast$                           \\ \hline
			sm-shapes~\cite{hoffmann2022synthmorph}               & A,SVF                   & Diff.                                 & DICE        & 79.8\resTS                 & 0.2981            & $\ast$                           \\
			sm-brains~\cite{hoffmann2022synthmorph}               & A,SVF                   & Diff.                                 & DICE        & 78.4\resTS                 & 0.0364            & $\ast$                           \\
			\multirow{2}{*}{\textbf{Ours} \emph{(std. protocol)}} & DVF                     & \cellcolor{black!10}\texttt{GradICON} & LNCC        & 78.7$\dagger$              & 0.0012            & $\ast$                           \\
			                                                      & DVF                     & \cellcolor{black!10}\texttt{GradICON} & LNCC        & \bf{80.5}$\ddagger$        & 0.0004            & $\ast$                           \\
			\midrule
			\multicolumn{6}{c}{\bf DirLab}                                                                                                                                                                                            \\ \midrule
			\bf Method                                            & \bf Trans.              & $\lreg$                               & $\lsim$     & \textbf{mTRE} $\downarrow$ & $\%|J|\downarrow$                                    \\[-2pt]
			                                                      &                         &                                       &             & {\footnotesize [mm]}       &                                                      \\[-2pt]
			Initial                                               &                         &                                       &             & 23.36\resTS                &                                                      \\ \midrule
			SyN~\cite{avants2008symmetric}                        & A,VF                    & Gaussian                              & LNCC        & 1.79\resTS                 & \textemdash       & \cite{hansen2021graphregnet}     \\
			Elastix~\cite{klein2009elastix}                       & A,B-Spline              & BE                                    & MSE         & 1.32\resTS                 & \textemdash       & \cite{hansen2021graphregnet}     \\
			NiftyReg~\cite{modat2010fast}                         & A,B-Spline              & BE                                    & MI          & 2.19\resTS                 & \textemdash       & \cite{hansen2021graphregnet}     \\
			PTVReg~\cite{vishnevskiy2017isotropic}                & DVF                     & TV                                    & LNCC        & 0.96\resTS                 & \textemdash       & ~\cite{vishnevskiy2017isotropic} \\
			RRN~\cite{he2021recursive}                            & DVF                     & TV                                    & LNCC        & \textbf{0.83}\resTS        & \textemdash       & ~\cite{he2021recursive}          \\
			\midrule
			VM~\cite{balakrishnan2019voxelmorph}                  & A,SVF                   & Diff.                                 & NCC         & 9.88\resTS                 & 0                 & $\ast$                           \\
			LapIRN~\cite{mok2020large}                            & SVF                     & Diff.                                 & NCC         & 2.92\resTS                 & 0                 & $\ast$                           \\
			LapIRN~\cite{mok2020large}                            & DVF                     & Diff.                                 & NCC         & 4.24\resTS                 & 0.0105            & $\ast$                           \\
			Hering et al. ~\cite{hering2021cnn}                   & DVF                     & Curv+VCC                              & DICE+KP+NGF & 2.00\resTS                 & 0.0600            & ~\cite{hering2021cnn}            \\
			GraphRegNet~\cite{hansen2021graphregnet}              & DV                      & \textemdash                           & MSE         & 1.34\resTS                 & \textemdash       & ~\cite{hansen2021graphregnet}    \\
			PLOSL~\cite{wang2022PLOSL}                            & DVF                     & Diff.                                 & TVD+VMD     & 3.84\resTS                 & 0                 & ~\cite{wang2022PLOSL}            \\
			PLOSL$_{50}$~\cite{wang2022PLOSL}                     & DVF                     & Diff.                                 & TVD+VMD     & 1.53\resTS                 & 0                 & ~\cite{wang2022PLOSL}            \\
			\texttt{ICON}~\cite{greer2021icon}                    & DVF                     & \texttt{ICON}                         & LNCC        & 7.04\resTS                 & 0.3792            & $\ast$                           \\
			\multirow{2}{*}{\textbf{Ours} \emph{(std. protocol)}} & DVF                     & \cellcolor{black!10}\texttt{GradICON} & LNCC        & 1.93$\dagger$              & 0.0003            & $\ast$                           \\
			                                                      & DVF                     & \cellcolor{black!10}\texttt{GradICON} & LNCC        & \textbf{1.31}$\ddagger$    & 0.0002            & $\ast$                           \\
			\bottomrule\vspace{-5mm}
		\end{tabular}
		\caption{Full comparison on \textbf{OAI}, \textbf{HCP} and \textbf{DirLab}. $\dagger$ and $\ddagger$ indicate results from our standard training protocol, without ($\dagger$) and with ($\ddagger$) instance optimization (\ref{subsection:training}). Only when \texttt{GradICON} is trained with MSE, we set $\lambda=0.2$. \emph{Top} and \emph{bottom} table parts denote non-learning and learning-based methods, resp. For \textbf{DirLab}, results are shown in the common \emph{inspiration$\rightarrow$expiration} direction. Results marked with $^\ast$ are reported by us using code from the official repository. \underline{A}: affine pre-registration, \underline{BE}: bending energy, \underline{MI}: mutual information, \underline{DV}: displacement vector of sparse key points, \underline{TV}: total variation, \underline{Curv}: curvature regularizer, \underline{VCC}: volume change control, \underline{NGF}: normalized gradient flow, \underline{TVD}: sum of squared tissue volume difference, \underline{VMD}: sum of squared vesselness measure difference, \underline{Diff}: diffusion, \underline{VF}: velocity field, \underline{SVF}: stationary VF, \underline{DVF}: displacement vector field. $\underline{\text{PLOSL}_{50}}$: 50 iterations of instance optimization with PLOSL.}
		\label{tab:exp_real_datasets_full}
	\end{small}
\end{table*}
In this section, we 1) present Table~\ref{tab:exp_real_datasets_full}, which is a complete version of Table~\ref{tab:exp_oai}, and 2) describe the experimental details for comparisons in Table~\ref{tab:exp_oai}.
\vskip1ex
\noindent
\textbf{sm-shapes and sm-brains.}
We evaluate the SynthMorph~\cite{hoffmann2022synthmorph} model with pre-trained weights from its official repository\footnote{\label{fn:voxelmorph}\url{https://github.com/voxelmorph/voxelmorph}} on the same \textbf{HCP} test set we use for \texttt{GradICON} and follow the suggested testing protocol from the repository. We first run \textbf{FreeSurfer-Affine} (see the following Freesurfer-Affine paragraph) to align the source and target image to the reference image provided in the repository. Then, we run \emph{SynthMorph-shapes} (\texttt{sm-shapes}) and \emph{SynthMorph-brains} (\texttt{sm-brains}) models to obtain the deformation between the pre-aligned source and target images. To compute the final transformation field used to warp the original source label map to the target label map, we first generate an identity map in the target image space and transform it via the target-to-reference affine matrix. Then, we compose the transformed map with the deformation field computed by SynthMorph. Lastly, we transform the composed deformation field via the reference-to-source affine matrix (obtained by inverting the source-to-reference affine matrix). Eventually, we use the final computed deformation field to warp the original source label map and then compute the DICE between the warped label map and the target label map in the \emph{original} target space.
\vskip1ex
\noindent
\textbf{FreeSurfer-Affine.}
We report the affine pre-alignment result from our SynthMorph experiment and label it as FreeSurfer-Affine~\cite{reuter2010highly} in Table~\ref{tab:exp_oai}. FreeSurfer is run with the configuration recommended in the SynthMorph repository. For evaluation purposes, we compose the target-to-reference affine matrix and reference-to-source affine matrix the same way as we did in the SynthMorph experiment except that we skip the step to compose the deformation field computed by SynthMorph. Essentially, we simply assume that the non-parametric part that would have been obtained by SynthMorph is set to the identity transform thereby only leaving the affine registrations. This experiment differs from directly obtaining an affine transformation between the source and the target spaces as instead we go through the template space and compute two affine transformations. However, this choice of affine transform composition allows a more direct assessment of the improvements obtained by SynthMorph.
\vskip1ex
\noindent
\textbf{\texttt{ICON}.}
We follow a similar design as described in \cite{greer2021icon} and, in particular, adopt the \texttt{tallUNet2} architecture as the backbone network. Specifically, a composition of two such UNet's is initially trained on half-resolution image pairs. This network is then composed with a third UNet, trained on full-resolution image pairs.
\vskip1ex
\noindent
\textbf{VoxelMorph.}
We use the official code from the VoxelMorph repository\ref{fn:voxelmorph} %\footnote{\url{https://github.com/voxelmorph/voxelmorph}} 
and train on \textbf{COPDGene}. As VoxelMorph requires pre-registration, we train another neural network for affine pre-registration. Table~\ref{tab:exp_vm_affine_preregistration} shows the registration accuracy of this affine registration network.
For VoxelMorph, we use NCC as the similarity measure, set the learning rate to 1e-3, and the regularizer weight to 5. We keep all other settings at the provided default values. %The network is trained on 4  RTX A6000 GPUs in parallel with a batch size of 12 for 100 epochs. 
Since the official code provides the inverse transformation, the bi-directional registration result is obtained with the forward map and its inverse map is computed by the official code.
\begin{table*}[htp]
	\centering
	\begin{small}
		\begin{tabular}{ccc}
			\toprule
			       & \bf mTRE\, $\downarrow$ & \bf DICE\, $\uparrow$ \\ \midrule
			Affine & 13.715                  & 80.23                 \\\bottomrule
		\end{tabular}
	\end{small}
	\caption{Registration performance measures of the pre-registration \emph{affine} network for our VoxelMorph comparison on \textbf{DirLab}.}
	\label{tab:exp_vm_affine_preregistration}
\end{table*}
\vskip1ex
\noindent
\textbf{LapIRN.}
We obtain the network from the official repository\footnote{\url{https://github.com/cwmok/LapIRN}} of LapIRN and train it on \textbf{COPDGene}. In particular, we train using the training script provided by the official repository with hyperparameter tuning for {\bf COPDGene} data. We switched from LNCC to NCC because we observed unstable training with the LNCC implementation provided in the official LapIRN repository. For each resolution, we adjust the learning rate and $\lambda$ to assure that the training converges. Table~\ref{tab:exp_lapirn_parameters} provides the hyperparameters we used to obtain the results in Table~\ref{tab:exp_oai}. We randomly swap the source and target images during training so that the trained network can work for bi-directional registration.
\begin{table*}[htp]
	\centering
	\begin{small}
		\begin{tabular}{ccccc}
			\toprule
			\multicolumn{1}{c}{\multirow{2}{*}{Resolution}} & \multicolumn{2}{c}{LapIRN (disp)} & \multicolumn{2}{c}{LapIRN (sVF)}                                  \\
			\cmidrule{2-5}
			\multicolumn{1}{c}{}                            & \multicolumn{1}{c}{\bf LR}        & \multicolumn{1}{c}{\bf reg. weight} & \bf LR    & \bf reg. weight \\ \midrule
			1                                               & $1e^{-4}$                         & 0.1                                 & $1e^{-4}$ & 0.1             \\
			$\frac{1}{2}$                                  & $5e^{-5}$                         & 0.1                                 & $1e^{-4}$ & 0.1             \\
			$\frac{1}{4}$                                  & $1e^{-5}$                         & 1                                   & $5e^{-5}$ & 1               \\ \bottomrule
		\end{tabular}
		\caption{\label{tab:exp_lapirn_parameters} Learning rate (LR) and regularization weight (reg. weight) hyperparameters of LapIRN per resolution.}
	\end{small}
\end{table*}

\subsection{Model statistics}\label{sec:model_statistics}
We compute model statistics regarding the number of parameters, peak memory
use, FLOPs, and inference time using built-in
PyTorch\footnote{\url{https://pytorch.org/}} functions and the
\texttt{thop}\footnote{\url{https://github.com/Lyken17/pytorch-OpCounter}}
package. This experiment is conducted using an NVIDIA GeForce RTX 3090 GPU
with a batch size of 1 and randomly generated image pairs of size
$175\times175\times175$. We run the model 10 times and take the average of the
elapsed time as the final measurement. In addition, the peak GPU memory usage
is reported for each model. Table~\ref{tab:model_statistics_lung} and
Table~\ref{tab:model_statistics_unet} list the statistics of models evaluated
in Table~\ref{tab:exp_oai} and the UNet used in our ablation study of
Sec.~\ref{subsection:ablationstudy}. Working on 2-D convolutional networks
builds a strong intuition that if, in a downsampling step, we double the
number of channels and cut in half the resolution, the amount of computation
stays roughly constant. This intuition is not correct for 3-D networks. In
fact, for 3-D networks, doubling the number of channels and halving the
resolution cuts the amount of computation by 1/2. As a result, large channel
counts deep in the network are, from a computation time and VRAM perspective,
free. The UNet from \texttt{ICON} approach~\cite{greer2021icon} takes
advantage of this effect to boost performance using a large parameter count
while reducing runtime and VRAM usage compared to the standard VoxelMorph
channel counts. We used the same approach for our registration networks using
\texttt{GradICON}. Note that Table~\ref{tab:model_statistics_lung} illustrates
that even though \texttt{ICON} and \texttt{GradICON} use about 50 times more
parameters than LapIRN and roughly 150 times more parameters than VoxelMorph,
memory consumption and inference times are in fact lower.
\begin{table*}[h!]
	\begin{small}
		\centering
		\vspace{\baselineskip}
		\begin{tabular}{lcccccc}\toprule
                                              & \multirow{2}{*}{\bf \#Params} & \multicolumn{3}{c}{Inference} & \multicolumn{2}{c}{Train} \\ \cmidrule{3-7}
			                                 &  & \textbf{Peak Mem.}~(MB) & \textbf{FLOPs} & \textbf{Time}~(ms) & \textbf{Peak Mem.}~(MB) & \textbf{Time}~(ms) \\ \midrule
			VM (SVF)                         & 327,331      & 4548                    & 397.878G       & 190.10  & \textemdash       & \textemdash            \\
			LapIRN (SVF)                     & 923,748      & 5578                    & 652.310G       & 253.87  & \textemdash       & \textemdash            \\
			LapIRN (DV)                      & 923.748      & 5576                    & 652.310G       & 235.30  & \textemdash       & \textemdash            \\
			\texttt{ICON}                    & 53,010,687   & 2918                    & 678.513G       & 96.36   & 8082              & 573.57                 \\
			\texttt{GradICON-Stage1}         & 53,010,687   & 2934                    & 618.592G       & 88.24   & 9384              & 727.77                 \\
			\texttt{GradICON-Stage1\&Stage2} & 70,680,916   & 3122                    & 1.159T         & 160.59  & 13482                & 1162.54                     \\
			\bottomrule
		\end{tabular}
		\caption{Model statistics at inference (test) time. \underline{G} denotes gigaFLOPS, \underline{T} denotes teraFLOPS.}
		\label{tab:model_statistics_lung}
	\end{small}
\end{table*}
\begin{table*}[htp]
	\centering
	\begin{small}
		\vspace{\baselineskip}
		\begin{tabular}{lccc}\toprule
			                                            & \bf \#Params & \textbf{Peak Mem.}~(MB) & \textbf{FLOPs} \\ \midrule
			UNet from~\cite{balakrishnan2019voxelmorph} & 327,331      & 4182.0                  & 397.878G       \\
			UNet from~\cite{greer2021icon}              & 17,670,229   & 2244                    & 540.084G       \\
			\bottomrule
		\end{tabular}
		\caption{Model statistics of the UNets used in our ablation study. \underline{G} denotes gigaFLOPS.}
		\label{tab:model_statistics_unet}
	\end{small}
\end{table*}

\section{Visualizations}
\label{appendix:visualizations}
In Fig.~\ref{Fig:knee_example_OAI0} and Fig.~\ref{Fig:knee_example_OAI1}, we show two example
registration cases from \textbf{OAI}, Fig.~\ref{Fig:lung_example_DirLab1} and Fig.~\ref{Fig:lung_example_DirLab2} show two example registration cases on \textbf{DirLab}, and Fig.~\ref{Fig:brain_example_case1} and Fig.~\ref{Fig:brain_example_case2} show two example
registration cases on \textbf{HCP}.

In Fig.~\ref{Fig:network_block_diagram} we show a block diagram of the network structure described in Sec.~\ref{sec:network_architecture}, that is more detailed.
\begin{figure*}
	\includegraphics[width=\textwidth]{Figures/medical_vis_cvpr-2/OAI0_reg_vis.pdf}
	\caption{Example registration case \textbf{A} (from test set instances) performed using \texttt{GradICON} and our standard training protocol ($\dagger$) w/o instance optimization on the \textbf{OAI} dataset. \emph{Best-viewed in color.}\label{Fig:knee_example_OAI0}}
\end{figure*}
\begin{figure*}
	\includegraphics[width=\textwidth]{Figures/medical_vis_cvpr-2/OAI1_reg_vis.pdf}
	\caption{Example registration case \textbf{B} (from test set instances) performed using \texttt{GradICON} and our standard training protocol ($\dagger$) w/o instance optimization on the \textbf{OAI} dataset. \emph{Best-viewed in color.}\label{Fig:knee_example_OAI1}}
\end{figure*}
\begin{figure*}
	\includegraphics[width=\textwidth]{Figures/medical_vis_cvpr-2/DirLab1_reg_vis.pdf}
	\caption{Example registrations case \textbf{A} performed using \texttt{GradICON} and our standard training protocol ($\dagger$) w/o instance optimization on the \textbf{Dirlab} dataset. \emph{Best-viewed in color.}\label{Fig:lung_example_DirLab1}}
\end{figure*}
\begin{figure*}
	\includegraphics[width=\textwidth]{Figures/medical_vis_cvpr-2/DirLab2_reg_vis.pdf}
	\caption{Example registrations case \textbf{B} performed using \texttt{GradICON} and our standard training protocol ($\dagger$) w/o instance optimization on the \textbf{Dirlab} dataset. \emph{Best-viewed in color.}\label{Fig:lung_example_DirLab2}}
\end{figure*}
\begin{figure*}
	\includegraphics[width=\textwidth]{Figures/medical_vis_cvpr-2/HCP_registration_visualizations_Case1.pdf}
	\caption{\label{Fig:brain_example_case1} Example registration case \textbf{A} (from test set instances) performed using \texttt{GradICON} and our standard training protocol ($\dagger$) w/o instance optimization on the \textbf{HCP} dataset. \emph{Best-viewed in color.}}
\end{figure*}
\begin{figure*}
	\includegraphics[width=\textwidth]{Figures/medical_vis_cvpr-2/HCP_registration_visualizations_Case2.pdf}
	\caption{\label{Fig:brain_example_case2} Example registration case \textbf{B} (from test set instances) performed using \texttt{GradICON} and our standard training protocol ($\dagger$) w/o instance optimization on the \textbf{HCP} dataset. \emph{Best-viewed in color.}}
\end{figure*}

\begin{figure*}
\centering
    \includegraphics[width=.7\textwidth]{Figures/GradICON2.pdf}
    \caption{Our approach is most succinctly described using equations, as done in \ref{sec:network_architecture}
, but we also desire to respect the convention that neural network papers include a representation of the network as a block diagram. Our "atomic," or simplest component registration network is a U-Net outputting a deformation (a). $\Phi^{AB}$, the output of this component, is a \emph{python function} that may be called on a tensor of coordinates. Components can be combined using the \texttt{TwoStep} and \texttt{Down} operators (b). The 'function composition' block in this row is implemented by the python code \texttt{lambda coords: phi\_AB(psi\_AB(coords))} , which is pleasing enough to justify our decision to represent deformations as functions. These parts are combined into the Stage1 and Stage2 networks we use for our general purpose registration approach (c). Finally, this network is regularized by a finite difference approximation of the gradient of the inverse consistency error (d)}
\label{Fig:network_block_diagram}
\end{figure*}
\section{Potential negative societal impacts}
\label{Sec:social_impact}
Image registration results might not be accurate or might even fail for certain image pairs in practice. Hence, careful quality control of the results should be performed when registrations are used for decision-support systems in a medical context.
